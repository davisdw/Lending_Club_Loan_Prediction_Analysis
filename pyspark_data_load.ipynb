{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davisdw/Lending_Tree_Loan_Prediction_Analysis/blob/main/pyspark_data_load.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing & Exporting CSV Data\n",
        "\n",
        "**Reads the CSV files, accepted & rejected loans from Amazon AWS s3 Bucket**\n",
        "\n",
        "**Reduce Un-needed Columns from both tables**\n",
        "\n",
        "**Review the dataset and perform data wrangling and cleaning**\n",
        "\n",
        "**Merge two datasets together**\n",
        "\n",
        "**Export the cleaned_df dataset back to s3 bucket to prep for running modeling, prediction and visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3\n",
        "import pandas as pd\n",
        "from io import StringIO # uses this library for data conversion\n",
        "import awsKeyConfig\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "# Use boto3 to get the object from S3\n",
        "# Retrieve the aws credential keys\n",
        "\n",
        "s3 = boto3.client('s3',\n",
        "aws_access_key_id=awsKeyConfig.keyID,\n",
        "aws_secret_access_key=awsKeyConfig.secretKey,\n",
        "region_name='us-east-1'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data Wrangling and Cleaning for Loan Accepted Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieves the Bucket Name and Key <file_name> and \n",
        "obj = s3.get_object(Bucket='davis-data-cloud-of-wonders', Key='accepted_2007_to_2018Q4.csv')\n",
        "data = obj['Body'].read().decode('utf-8')\n",
        "\n",
        "# Validates whether the connection to s3 is successfull or fail \n",
        "status = obj.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
        "\n",
        "if status == 200:\n",
        "    print(f\"Successful S3 put_object response. Status - {status}\")\n",
        "else:\n",
        "    print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n",
        "\n",
        "\n",
        "# Use StringIO to convert the string data to a file-like object\n",
        "data_file = StringIO(data)\n",
        "\n",
        "# Create a DataFrame from the CSV data\n",
        "accepted_df = pd.read_csv(data_file)\n",
        "\n",
        "accepted_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# view number of columns and rows\n",
        "accepted_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# selected the required columns needed for analysis\n",
        "clean_accept_df = accepted_df[[\"loan_amnt\",\n",
        "                    \"term\",\n",
        "                    \"int_rate\",\n",
        "                    \"installment\",\n",
        "                    \"grade\",\n",
        "                    \"sub_grade\",\n",
        "                    \"emp_title\",\n",
        "                    \"emp_length\",\n",
        "                    \"home_ownership\",\n",
        "                    \"annual_inc\",\n",
        "                    \"verification_status\",\n",
        "                    \"issue_d\",\n",
        "                    \"loan_status\",\n",
        "                    \"purpose\",\n",
        "                    \"addr_state\",\n",
        "                    \"dti\",\n",
        "                    \"fico_range_low\",\n",
        "                    \"fico_range_high\" ]]\n",
        "\n",
        "clean_accept_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# view the dataset shape after removal of un-needed columns:\n",
        "clean_accept_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# view full display of the data types for the accepted table\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(clean_accept_df.dtypes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the number of unique values in each column\n",
        "for a in clean_accept_df:\n",
        "    print(a, len(clean_accept_df[a].unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "**Data Wrangling and Cleaning for Loan Rejected Dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reads in the rejected data : \n",
        "\n",
        "obj = s3.get_object(Bucket='davis-data-cloud-of-wonders', Key='rejected_2007_to_2018Q4.csv')\n",
        "data = obj['Body'].read().decode('utf-8')\n",
        "\n",
        "# Validates whether the connection to s3 is successfull or fail \n",
        "status = obj.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
        "\n",
        "if status == 200:\n",
        "    print(f\"Successful S3 put_object response. Status - {status}\")\n",
        "else:\n",
        "    print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n",
        "\n",
        "# Use StringIO to convert the string data to a file-like object\n",
        "data_file = StringIO(data)\n",
        "\n",
        "# Create a DataFrame from the CSV data\n",
        "rejected_df = pd.read_csv(data_file)\n",
        "\n",
        "rejected_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# view number of columns and rows\n",
        "rejected_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dropping policy_code column\n",
        "clean_reject_df = rejected_df.drop(columns=['policy_code'])\n",
        "clean_reject_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rename some of the columns to match the clean_accept_df dataframe\n",
        "clean_reject_df.rename(columns={...}, inplace= True) # ADD IN COLUMNS TO RENAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# view the dataset shape after removal of un-needed columns:\n",
        "clean_reject_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(clean_reject_df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for b in clean_reject_df:\n",
        "    print(b, len(clean_reject_df[b].unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "**Merging two datasets**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# quick review of the clean_accept_df dataset\n",
        "clean_accept_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# quick review of the clean_reject_df dataset\n",
        "clean_reject_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add an column to both tables that determines if applicant had approval of the loan  \n",
        "# using boolean values \"0\" for those who were approved (accept_df) and \"1\" for denied (reject_df) for loan\n",
        "clean_accept_df['is_approve'] = np.where(...) # insert condition and set 0 as True\n",
        "clean_reject_df['is_approve'] = np.where(...) # insert condition and set 1 as False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Joined both datasets together, \"concat\" using join inner on \"loan_request_id\" column \n",
        "joined_loan_df = pd.merge(clean_accept_df, clean_reject_df)\n",
        "joined_loan_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an index for the combined dataframes as unique identifier\n",
        "joined_loan_df['loan_app_id'] = range(1, len(clean_reject_df) + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Write Output files back to s3**\n",
        "\n",
        "--Once the dataset is formatted wrangled and cleaned we're outputting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Name</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [ID, Name, Age]\n",
              "Index: []"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# This is an dummy dataframe, i've created to test to see if able to write data into AWS s3 output_file folder \n",
        "\n",
        "\"\"\" \n",
        "data1 = {'ID': [1, 2, 3, 4], 'Name': ['tom','nick','juli','solyiah'], 'Age' : [10, 15, 14, 10]}\n",
        "data2 = {'ID': [5, 6, 7, 8], 'Name': ['dick', 'joe', 'harry', 'jake'], 'Age': [21, 30, 45, 30]}\n",
        "\n",
        "test_df_1 = pd.DataFrame(data1)\n",
        "test_df_2 = pd.DataFrame(data2)\n",
        "\n",
        "joined_test_df = pd.merge(test_df_1, test_df_2)\n",
        "\n",
        "joined_test_df.head()\n",
        "\n",
        "# test_df_2.head()\n",
        "# test_df_1.head()\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# variables for buck name and path to save the output file to\n",
        "bucket_name = \"davis-data-cloud-of-wonders\"\n",
        "path = \"output/output_data.csv\"\n",
        "\n",
        "# converts string into file format before saving the file\n",
        "with io.StringIO() as csv_buffer:\n",
        "    joined_loan_df.to_csv(csv_buffer, index=False)\n",
        "\n",
        "    # place file into the specified buck and path location in the s3 directory\n",
        "    response = s3.put_object(\n",
        "        Bucket= bucket_name, Key=path, Body=csv_buffer.getvalue()\n",
        "    )\n",
        "\n",
        "    # Validates whether the connection to s3 is successfull or fail \n",
        "    status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
        "\n",
        "    if status == 200:\n",
        "        print(f\"Successful S3 put_object response. Status - {status}\")\n",
        "    else:\n",
        "        print(f\"Unsuccessful S3 put_object response. Status - {status}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
